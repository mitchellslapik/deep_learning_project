{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141ba661-a92d-40ce-b338-7a2071e26a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3ec60fd1e34e82b80161ffdf894d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Training Loss: 1.5572, Training Accuracy: 0.4290\n",
      "Testing Loss: 1.3526, Testing Accuracy: 0.5034\n",
      "Epoch 2/10, Training Loss: 1.2954, Training Accuracy: 0.5399\n",
      "Testing Loss: 1.2676, Testing Accuracy: 0.5507\n",
      "Epoch 3/10, Training Loss: 1.1848, Training Accuracy: 0.5815\n",
      "Testing Loss: 1.2438, Testing Accuracy: 0.5647\n",
      "Epoch 4/10, Training Loss: 1.1073, Training Accuracy: 0.6103\n",
      "Testing Loss: 1.1862, Testing Accuracy: 0.5850\n",
      "Epoch 5/10, Training Loss: 1.0571, Training Accuracy: 0.6287\n",
      "Testing Loss: 1.1508, Testing Accuracy: 0.6109\n",
      "Epoch 6/10, Training Loss: 1.0205, Training Accuracy: 0.6423\n",
      "Testing Loss: 1.1168, Testing Accuracy: 0.6224\n",
      "Epoch 7/10, Training Loss: 0.9893, Training Accuracy: 0.6525\n",
      "Testing Loss: 1.1722, Testing Accuracy: 0.6009\n",
      "Epoch 8/10, Training Loss: 0.9693, Training Accuracy: 0.6599\n",
      "Testing Loss: 1.1109, Testing Accuracy: 0.6219\n",
      "Epoch 9/10, Training Loss: 0.9475, Training Accuracy: 0.6658\n",
      "Testing Loss: 1.1318, Testing Accuracy: 0.6224\n",
      "Epoch 10/10, Training Loss: 0.9262, Training Accuracy: 0.6771\n",
      "Testing Loss: 1.1496, Testing Accuracy: 0.6122\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet-5 model without dropout for color images\n",
    "class LeNet5Color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5Color, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset for color images\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize LeNet-5 model for color images\n",
    "net_color = LeNet5Color()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_color = nn.CrossEntropyLoss()\n",
    "optimizer_color = optim.Adam(net_color.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training/test accuracy and loss for color images\n",
    "train_accuracy_list_color = []\n",
    "test_accuracy_list_color = []\n",
    "train_loss_list_color = []\n",
    "test_loss_list_color = []\n",
    "\n",
    "# Train the model for color images\n",
    "for epoch in range(10):  # Change the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_color.zero_grad()\n",
    "        outputs = net_color(inputs)\n",
    "        loss = criterion_color(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_color.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch {epoch+1}/{10}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Save training accuracy and loss\n",
    "    train_accuracy_list_color.append(train_accuracy)\n",
    "    train_loss_list_color.append(train_loss)\n",
    "\n",
    "    # Test the model for color images\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = net_color(inputs)\n",
    "            loss = criterion_color(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    test_loss /= len(testloader)\n",
    "    print(f'Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Save test accuracy and loss\n",
    "    test_accuracy_list_color.append(test_accuracy)\n",
    "    test_loss_list_color.append(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019421da-0a9e-41f6-a5aa-cb426ba5539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Show the first layer filters for color images using make_grid\n",
    "filter_weights = net_color.conv1.weight.data.clone()\n",
    "filter_weights -= filter_weights.min()\n",
    "filter_weights /= filter_weights.max()\n",
    "\n",
    "grid = vutils.make_grid(filter_weights, nrow=6, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bae71e7-4113-4e41-a925-96c69d5d23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for color images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save intermediate activations from the first layer for color images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_train5.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_test5.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "# Save training and test accuracy and loss for color images\n",
    "np.save('train_accuracy_color5.npy', np.array(train_accuracy_list_color))\n",
    "np.save('test_accuracy_color5.npy', np.array(test_accuracy_list_color))\n",
    "np.save('train_loss_color5.npy', np.array(train_loss_list_color))\n",
    "np.save('test_loss_color5.npy', np.array(test_loss_list_color))\n",
    "\n",
    "print('Finished Training for color images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f87141-39e1-4e86-8793-8cc014418f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_color.state_dict(), 'lenet5_color_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c421-24a4-43a8-b695-c0c1235c4f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
