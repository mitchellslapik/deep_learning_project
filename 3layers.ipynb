{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141ba661-a92d-40ce-b338-7a2071e26a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Training Loss: 1.4925, Training Accuracy: 0.4642\n",
      "Testing Loss: 1.3116, Testing Accuracy: 0.5356\n",
      "Epoch 2/10, Training Loss: 1.2832, Training Accuracy: 0.5464\n",
      "Testing Loss: 1.2471, Testing Accuracy: 0.5597\n",
      "Epoch 3/10, Training Loss: 1.2212, Training Accuracy: 0.5717\n",
      "Testing Loss: 1.2244, Testing Accuracy: 0.5762\n",
      "Epoch 4/10, Training Loss: 1.1904, Training Accuracy: 0.5834\n",
      "Testing Loss: 1.1651, Testing Accuracy: 0.5932\n",
      "Epoch 5/10, Training Loss: 1.1646, Training Accuracy: 0.5935\n",
      "Testing Loss: 1.1829, Testing Accuracy: 0.5869\n",
      "Epoch 6/10, Training Loss: 1.1477, Training Accuracy: 0.6006\n",
      "Testing Loss: 1.1613, Testing Accuracy: 0.5969\n",
      "Epoch 7/10, Training Loss: 1.1346, Training Accuracy: 0.6058\n",
      "Testing Loss: 1.1759, Testing Accuracy: 0.5901\n",
      "Epoch 8/10, Training Loss: 1.1260, Training Accuracy: 0.6068\n",
      "Testing Loss: 1.1667, Testing Accuracy: 0.5950\n",
      "Epoch 9/10, Training Loss: 1.1167, Training Accuracy: 0.6114\n",
      "Testing Loss: 1.1687, Testing Accuracy: 0.5924\n",
      "Epoch 10/10, Training Loss: 1.1113, Training Accuracy: 0.6152\n",
      "Testing Loss: 1.1851, Testing Accuracy: 0.5915\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet-5 model without dropout for color images\n",
    "class LeNet5Color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5Color, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 10)\n",
    "        #self.fc2 = nn.Linear(120, 10)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset for color images\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize LeNet-5 model for color images\n",
    "net_color = LeNet5Color()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_color = nn.CrossEntropyLoss()\n",
    "optimizer_color = optim.Adam(net_color.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training/test accuracy and loss for color images\n",
    "train_accuracy_list_color = []\n",
    "test_accuracy_list_color = []\n",
    "train_loss_list_color = []\n",
    "test_loss_list_color = []\n",
    "\n",
    "# Train the model for color images\n",
    "for epoch in range(10):  # Change the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_color.zero_grad()\n",
    "        outputs = net_color(inputs)\n",
    "        loss = criterion_color(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_color.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch {epoch+1}/{10}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Save training accuracy and loss\n",
    "    train_accuracy_list_color.append(train_accuracy)\n",
    "    train_loss_list_color.append(train_loss)\n",
    "\n",
    "    # Test the model for color images\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = net_color(inputs)\n",
    "            loss = criterion_color(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    test_loss /= len(testloader)\n",
    "    print(f'Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Save test accuracy and loss\n",
    "    test_accuracy_list_color.append(test_accuracy)\n",
    "    test_loss_list_color.append(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019421da-0a9e-41f6-a5aa-cb426ba5539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Show the first layer filters for color images using make_grid\n",
    "filter_weights = net_color.conv1.weight.data.clone()\n",
    "filter_weights -= filter_weights.min()\n",
    "filter_weights /= filter_weights.max()\n",
    "\n",
    "grid = vutils.make_grid(filter_weights, nrow=6, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bae71e7-4113-4e41-a925-96c69d5d23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for color images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save intermediate activations from the first layer for color images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_train3.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_test3.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "# Save training and test accuracy and loss for color images\n",
    "np.save('train_accuracy_color3.npy', np.array(train_accuracy_list_color))\n",
    "np.save('test_accuracy_color3.npy', np.array(test_accuracy_list_color))\n",
    "np.save('train_loss_color3.npy', np.array(train_loss_list_color))\n",
    "np.save('test_loss_color3.npy', np.array(test_loss_list_color))\n",
    "\n",
    "print('Finished Training for color images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f87141-39e1-4e86-8793-8cc014418f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_color.state_dict(), 'lenet3_color_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c421-24a4-43a8-b695-c0c1235c4f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
