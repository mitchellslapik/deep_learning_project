{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141ba661-a92d-40ce-b338-7a2071e26a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Training Loss: 1.4943, Training Accuracy: 0.4548\n",
      "Testing Loss: 1.3086, Testing Accuracy: 0.5342\n",
      "Epoch 2/10, Training Loss: 1.2331, Training Accuracy: 0.5638\n",
      "Testing Loss: 1.2308, Testing Accuracy: 0.5651\n",
      "Epoch 3/10, Training Loss: 1.1381, Training Accuracy: 0.5976\n",
      "Testing Loss: 1.2191, Testing Accuracy: 0.5786\n",
      "Epoch 4/10, Training Loss: 1.0735, Training Accuracy: 0.6223\n",
      "Testing Loss: 1.1399, Testing Accuracy: 0.6040\n",
      "Epoch 5/10, Training Loss: 1.0266, Training Accuracy: 0.6377\n",
      "Testing Loss: 1.1616, Testing Accuracy: 0.6065\n",
      "Epoch 6/10, Training Loss: 0.9929, Training Accuracy: 0.6493\n",
      "Testing Loss: 1.1264, Testing Accuracy: 0.6104\n",
      "Epoch 7/10, Training Loss: 0.9581, Training Accuracy: 0.6631\n",
      "Testing Loss: 1.1844, Testing Accuracy: 0.6037\n",
      "Epoch 8/10, Training Loss: 0.9339, Training Accuracy: 0.6697\n",
      "Testing Loss: 1.1471, Testing Accuracy: 0.6098\n",
      "Epoch 9/10, Training Loss: 0.9069, Training Accuracy: 0.6790\n",
      "Testing Loss: 1.1526, Testing Accuracy: 0.6200\n",
      "Epoch 10/10, Training Loss: 0.8886, Training Accuracy: 0.6868\n",
      "Testing Loss: 1.1897, Testing Accuracy: 0.6179\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet-5 model without dropout for color images\n",
    "class LeNet5Color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5Color, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset for color images\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize LeNet-5 model for color images\n",
    "net_color = LeNet5Color()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_color = nn.CrossEntropyLoss()\n",
    "optimizer_color = optim.Adam(net_color.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training/test accuracy and loss for color images\n",
    "train_accuracy_list_color = []\n",
    "test_accuracy_list_color = []\n",
    "train_loss_list_color = []\n",
    "test_loss_list_color = []\n",
    "\n",
    "# Train the model for color images\n",
    "for epoch in range(10):  # Change the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_color.zero_grad()\n",
    "        outputs = net_color(inputs)\n",
    "        loss = criterion_color(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_color.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch {epoch+1}/{10}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Save training accuracy and loss\n",
    "    train_accuracy_list_color.append(train_accuracy)\n",
    "    train_loss_list_color.append(train_loss)\n",
    "\n",
    "    # Test the model for color images\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = net_color(inputs)\n",
    "            loss = criterion_color(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    test_loss /= len(testloader)\n",
    "    print(f'Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Save test accuracy and loss\n",
    "    test_accuracy_list_color.append(test_accuracy)\n",
    "    test_loss_list_color.append(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019421da-0a9e-41f6-a5aa-cb426ba5539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Show the first layer filters for color images using make_grid\n",
    "filter_weights = net_color.conv1.weight.data.clone()\n",
    "filter_weights -= filter_weights.min()\n",
    "filter_weights /= filter_weights.max()\n",
    "\n",
    "grid = vutils.make_grid(filter_weights, nrow=6, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bae71e7-4113-4e41-a925-96c69d5d23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for color images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save intermediate activations from the first layer for color images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_train4.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "first_layer_activations_color = net_color.conv1(images)\n",
    "np.save('first_layer_activations_test4.npy', first_layer_activations_color.detach().numpy())\n",
    "\n",
    "# Save training and test accuracy and loss for color images\n",
    "np.save('train_accuracy_color4.npy', np.array(train_accuracy_list_color))\n",
    "np.save('test_accuracy_color4.npy', np.array(test_accuracy_list_color))\n",
    "np.save('train_loss_color4.npy', np.array(train_loss_list_color))\n",
    "np.save('test_loss_color4.npy', np.array(test_loss_list_color))\n",
    "\n",
    "print('Finished Training for color images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f87141-39e1-4e86-8793-8cc014418f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_color.state_dict(), 'lenet4_color_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c421-24a4-43a8-b695-c0c1235c4f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
